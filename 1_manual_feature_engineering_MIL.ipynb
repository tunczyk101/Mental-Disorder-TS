{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320837f7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "18e18bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:36.052571Z",
     "start_time": "2025-01-15T20:45:36.044571Z"
    }
   },
   "source": [
    "import glob\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import warnings\n",
    "from lightgbm import LGBMClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "from utils import variance_thresholding, standardize, calculate_metrics, calculate_metrics_statistics, DatasetWin, calculate_metrics_from_df\n",
    "import lightgbm as lgb"
   ],
   "outputs": [],
   "execution_count": 199
  },
  {
   "cell_type": "code",
   "id": "b84c8514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:06.736564Z",
     "start_time": "2025-01-15T20:47:06.722564Z"
    }
   },
   "source": [
    "# parameters for Welch's method for estimating power spectrum\n",
    "\n",
    "NPERSEG = 60                    # length of segment\n",
    "NOVERLAP = int(0.75 * NPERSEG)  # overlap of segments\n",
    "NFFT = NPERSEG                  # length of FFT\n",
    "WINDOW = \"hann\"                 # window function type\n",
    "\n",
    "# parameters for saving data\n",
    "PROCESSED_DATA_DIR = \"processed_data\"\n",
    "DEPRESJON_PREFIX = \"depresjon\"\n",
    "PSYKOSE_PREFIX = \"psykose\"\n",
    "HYPERAKTIV_PREFIX = \"hyperaktiv\"\n",
    "MAIN_RESULTS_DIR = \"results\"\n",
    "DAY_WINDOWS_DIR = \"day_windows\"\n",
    "WINDOWS_DIR = \"windows\"\n",
    "DAY_NIGHT_HOURS = (8, 21) # (6, 22) / (8, 21)\n",
    "day_night_format = f'{DAY_NIGHT_HOURS[0]}_{DAY_NIGHT_HOURS[1]}' # \"6, 22\" / \"8, 21\""
   ],
   "outputs": [],
   "execution_count": 216
  },
  {
   "cell_type": "markdown",
   "id": "44ef0d95",
   "metadata": {},
   "source": [
    "# Manual feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9515d10",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "657c3d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:39.740218Z",
     "start_time": "2025-01-15T20:45:39.710218Z"
    }
   },
   "source": [
    "def basic_data_cleaning(data: List[List[pd.DataFrame]]) -> List[List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - Format \"timestamp\" to YYYY-MM-DD HH:MM:SS\n",
    "    - Drop redundant \"date\" column\n",
    "    - Convert \"activity\" to float32\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    for patient in data:\n",
    "        for df in patient:\n",
    "            # Convert and enforce the desired timestamp format\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], dayfirst=False)\n",
    "            df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            # Drop \"date\" column if it exists\n",
    "            if \"date\" in df.columns:\n",
    "                df.drop(\"date\", axis=1, inplace=True)\n",
    "            \n",
    "            # Ensure \"activity\" column is float32\n",
    "            df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that\n",
    "    correspond to the chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :param hour_day_start: hour when day start (default 8)\n",
    "    :param hour_day_end: hour when day ends (starts night) (default 21)\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= DAY_NIGHT_HOURS[0]) &\n",
    "                    (df[\"timestamp\"].dt.hour < DAY_NIGHT_HOURS[1])]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= DAY_NIGHT_HOURS[1]) |\n",
    "                    (df[\"timestamp\"].dt.hour < DAY_NIGHT_HOURS[0])]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_activity(df: pd.DataFrame, freq: str = \"min\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fill missing activity values by resampling based on given frequency.\n",
    "    \n",
    "    :param df: DataFrame with 'timestamp' and 'activity' columns.\n",
    "    :param freq: Resampling frequency (default: minute).\n",
    "    :return: DataFrame with missing values filled.\n",
    "    \"\"\"\n",
    "    df = df.copy() # create copy to avoid side effects\n",
    "  \n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "    df.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    # resample to the basic frequency, i.e. minute; this will create NaNs for\n",
    "    # any rows that may be missing\n",
    "    df = df.resample(freq).mean()\n",
    "    \n",
    "    # recreate index and \"timestamp\" column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # fill any NaNs with mean activity value\n",
    "    df[\"activity\"] = df[\"activity\"].fillna(df[\"activity\"].mean())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def resample(df: pd.DataFrame, freq: str = \"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resamples time series DataFrame with given frequency, aggregating each\n",
    "    segment with a mean.\n",
    "\n",
    "    :param df: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency passed to Pandas resample() function\n",
    "    :returns: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # group with given frequency\n",
    "    df = df.resample(freq, on=\"timestamp\").mean()\n",
    "\n",
    "    # recreate \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def proportion_of_zeros(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates proportion of zeros in given array, i.e. number of zeros divided\n",
    "    by length of array.\n",
    "    \n",
    "    :param x: 1D Numpy array\n",
    "    :returns: proportion of zeros\n",
    "    \"\"\"\n",
    "    # we may be dealing with floating numbers, we can't use direct comparison\n",
    "    zeros_count = np.sum(np.isclose(x, 0))\n",
    "    return zeros_count / len(x)\n",
    "\n",
    "\n",
    "def power_spectral_density(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates power spectral density (PSD) from \"activity\" column of a\n",
    "    DataFrame.\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: 1D Numpy array with power spectral density\n",
    "    \"\"\"\n",
    "\n",
    "    activity = df[\"activity\"].values\n",
    "    nperseg = min(NPERSEG, len(activity))  # Ensure nperseg doesn't exceed data length\n",
    "    noverlap = int(0.75 * nperseg) \n",
    "    \n",
    "    psd = scipy.signal.welch(\n",
    "        x=activity,\n",
    "        fs=(1/60),\n",
    "        nperseg=nperseg,\n",
    "        noverlap=noverlap,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"density\"\n",
    "    )[1]\n",
    "    return psd\n",
    "\n",
    "\n",
    "def spectral_flatness(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates spectral flatness of a signal, i.e. a geometric mean of the\n",
    "    power spectrum divided by the arithmetic mean of the power spectrum.\n",
    "    \n",
    "    If some frequency bins in the power spectrum are close to zero, they are\n",
    "    removed prior to calculation of spectral flatness to avoid calculation of\n",
    "    log(0).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: spectral flatness value\n",
    "    \"\"\"\n",
    "\n",
    "    activity = df[\"activity\"].values\n",
    "    nperseg = min(NPERSEG, len(activity))  # Ensure nperseg doesn't exceed data length\n",
    "    noverlap = int(0.75 * nperseg) \n",
    "\n",
    "    power_spectrum = scipy.signal.welch(\n",
    "        activity,\n",
    "        fs=(1/60),\n",
    "        nperseg=nperseg,\n",
    "        noverlap=noverlap,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"spectrum\"\n",
    "    )[1]\n",
    "    \n",
    "    non_zeros_mask = ~np.isclose(power_spectrum, 0)\n",
    "    power_spectrum = power_spectrum[non_zeros_mask]\n",
    "    \n",
    "    return scipy.stats.gmean(power_spectrum) / power_spectrum.mean()"
   ],
   "outputs": [],
   "execution_count": 200
  },
  {
   "cell_type": "markdown",
   "id": "7c465a53",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "id": "16826d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:40.069324Z",
     "start_time": "2025-01-15T20:45:40.051158Z"
    }
   },
   "source": [
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in time domain.\n",
    "    \n",
    "    :param df_resampled: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = df[\"activity\"].values\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X, ddof=1),  # apply Bessel's correction\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"proportion_of_zeros\": proportion_of_zeros(X)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ],
   "outputs": [],
   "execution_count": 201
  },
  {
   "cell_type": "code",
   "id": "9589aaf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:40.547489Z",
     "start_time": "2025-01-15T20:45:40.536490Z"
    }
   },
   "source": [
    "def extract_frequency_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in frequency domain, i.e. calculated\n",
    "    from its Power Spectral Density (PSD).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = power_spectral_density(df)\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X),\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"spectral_flatness\": spectral_flatness(df)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ],
   "outputs": [],
   "execution_count": 202
  },
  {
   "cell_type": "code",
   "id": "e9867a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:41.147811Z",
     "start_time": "2025-01-15T20:45:41.134811Z"
    }
   },
   "source": [
    "def extract_features_for_dataframes(dfs: List[List[pd.DataFrame]], is_condition: bool = True, freq: str = \"H\") \\\n",
    "        -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculates time and frequency features for given DataFrames. Uses given\n",
    "    frequency for resampling.\n",
    "    \n",
    "    Calculates features separately for:\n",
    "    - full 24hs\n",
    "    - days: default [8:00, 21:00)\n",
    "    - nights: default [21:00, 8:00)\n",
    "    \n",
    "    :param dfs: list of lists of DataFrames to extract features from; each one has to\n",
    "    have \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency\n",
    "    :returns: dictionary with keys \"full_24h\", \"day\" and \"night\", corresponding\n",
    "    to features from given parts of day\n",
    "    \"\"\"\n",
    "    full_dfs = basic_data_cleaning(dfs)\n",
    "    full_dfs = [[fill_missing_activity(df) for df in patient] for patient in full_dfs]\n",
    "    full_dfs = [[resample(df, freq=freq) for df in patient] for patient in full_dfs]\n",
    "    night_dfs = [[get_day_part(df, part=\"night\") for df in patient] for patient in full_dfs]\n",
    "    day_dfs = [[get_day_part(df, part=\"day\") for df in patient] for patient in full_dfs]\n",
    "\n",
    "    datasets = {}\n",
    "    \n",
    "    \n",
    "    for part, list_of_dfs in [(\"full_24h\", full_dfs), (\"night\", night_dfs), (\"day\", day_dfs)]:\n",
    "        features = []\n",
    "        for patient in range(len(list_of_dfs)):\n",
    "            for day in range(len(list_of_dfs[patient])):\n",
    "                time_features = extract_time_features(list_of_dfs[patient][day])\n",
    "                freq_features = extract_frequency_features(list_of_dfs[patient][day])\n",
    "    \n",
    "                merged_features = pd.merge(\n",
    "                    time_features,\n",
    "                    freq_features,\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                    suffixes=(\"_time\", \"_freq\")\n",
    "                )\n",
    "                merged_features['day'] = day + 1\n",
    "                merged_features['patient_id'] = patient + 1\n",
    "                if is_condition:\n",
    "                    merged_features['class'] = 1\n",
    "                else:\n",
    "                    merged_features['class'] = 0\n",
    "                features.append(merged_features)\n",
    "    \n",
    "        datasets[part] = pd.concat(features)\n",
    "        datasets[part].reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return datasets"
   ],
   "outputs": [],
   "execution_count": 203
  },
  {
   "cell_type": "markdown",
   "id": "af525f6b",
   "metadata": {},
   "source": [
    "## Hyperaktiv"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea71a0c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:13.942601Z",
     "start_time": "2025-01-15T20:47:13.089580Z"
    }
   },
   "source": [
    "path = os.path.join(PROCESSED_DATA_DIR, DAY_WINDOWS_DIR, \"hyperaktiv\")\n",
    "dataset = DatasetWin(dirpath=path, sep=',')\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ],
   "outputs": [],
   "execution_count": 217
  },
  {
   "cell_type": "code",
   "id": "4109cc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:25.659333Z",
     "start_time": "2025-01-15T20:47:13.954600Z"
    }
   },
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, is_condition=True, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, is_condition=False, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "    max_patient = condition_df['patient_id'].max()\n",
    "    control_df['patient_id'] += max_patient # changing numeration of patients\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "    datasets[part] = entire_df"
   ],
   "outputs": [],
   "execution_count": 218
  },
  {
   "cell_type": "code",
   "id": "4932f103",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:25.784326Z",
     "start_time": "2025-01-15T20:47:25.754329Z"
    }
   },
   "source": [
    "# save windows features\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR), exist_ok=True)\n",
    "for part, df in datasets.items():\n",
    "    filename = f\"{HYPERAKTIV_PREFIX}_{DAY_NIGHT_HOURS[0]}_{DAY_NIGHT_HOURS[1]}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    df.to_csv(filepath, index=False, header=True)"
   ],
   "outputs": [],
   "execution_count": 219
  },
  {
   "cell_type": "markdown",
   "id": "e195102c",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff79d6",
   "metadata": {},
   "source": [
    "## Classifiers, parameters, constants"
   ]
  },
  {
   "cell_type": "code",
   "id": "00c0242a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:38:52.136162Z",
     "start_time": "2025-01-15T19:38:52.123655Z"
    }
   },
   "source": [
    "classifiers = {\n",
    "    \"GBM\": LGBMClassifier(\n",
    "        objective='binary',\n",
    "        metric=['auc', 'binary_logloss'],\n",
    "        force_col_wise=True,\n",
    "        verbosity=-1\n",
    "    ),\n",
    "    \"LR\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        random_state=0,\n",
    "        solver=\"saga\",\n",
    "        max_iter=5000\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"poly\",\n",
    "        cache_size=512,\n",
    "        max_iter=5000\n",
    "    ),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        criterion=\"entropy\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    \"GBM\":{\n",
    "        'num_leaves': [25, 50, 100, 250, 500],\n",
    "        'max_depth': [5, 10, 15, 20, 25],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'reg_alpha': [0, 0.1, 0.2, 0.3],\n",
    "    },\n",
    "    \"LR\": {\n",
    "        \"C\": [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 100, 500, 1000],\n",
    "        \"class_weight\": [None, \"balanced\"],\n",
    "        \"l1_ratio\": [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                     0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"gamma\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 197
  },
  {
   "cell_type": "markdown",
   "id": "44383487",
   "metadata": {},
   "source": [
    "## Hyperaktiv Classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "299853bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:56.580565Z",
     "start_time": "2025-01-15T20:45:56.565566Z"
    }
   },
   "source": [
    "dataset = HYPERAKTIV_PREFIX"
   ],
   "outputs": [],
   "execution_count": 207
  },
  {
   "cell_type": "code",
   "id": "518e18e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:56.751570Z",
     "start_time": "2025-01-15T20:45:56.674566Z"
    }
   },
   "source": [
    "# read dataset\n",
    "# create dictionary with data split for night/day/all and also 6/22, 8/21 day/night\n",
    "datasets = {}\n",
    "    \n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    filename = f\"{dataset}_{day_night_format}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).dropna()"
   ],
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:45:59.164387Z",
     "start_time": "2025-01-15T20:45:56.847566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_directory = os.path.join(\".\", MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"hyperactiv\")\n",
    "predictions_directory = os.path.join('.', MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"hyperactiv\", \"predictions\")\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "os.makedirs(predictions_directory, exist_ok=True)\n",
    "\n",
    "for part in [\"night\", \"full_24h\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    X = datasets[part]\n",
    "    y = datasets[part]['class']\n",
    "    print(len(X))\n",
    "    info = X.iloc[:, -3:]\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "    fold_num = 0\n",
    "    all_predictions = pd.DataFrame()\n",
    "    \n",
    "    for train_idx, test_idx in group_kfold.split(X, y, groups=X['patient_id']):\n",
    "        fold_num += 1 \n",
    "        X = X.iloc[:, :-3]\n",
    "        print(\"fold: \", fold_num)\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.005)\n",
    "        X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "        for clf_type in [\"GBM\", \"LR\", \"SVM\", \"RF\"]: \n",
    "            if clf_type == \"GBM\":\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "            print(f\"    {clf_type}\")\n",
    "            \n",
    "            test_scores = []\n",
    "            \n",
    "            model = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=list(GroupKFold(n_splits=5).split(X_train, y_train, info.iloc[train_idx][\"patient_id\"]))\n",
    "            )\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics = calculate_metrics(model, X_test, y_test)\n",
    "            # print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "    \n",
    "            # Save individual fold metrics\n",
    "            pd.DataFrame.from_records(test_scores).to_csv(\n",
    "                os.path.join(results_directory, f\"test_scores_{day_night_format}_{part}_fold_{clf_type}\"),\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            predictions_dict = {\n",
    "                \"fold\": [fold_num] * len(y_test),\n",
    "                'classifier': [clf_type] * len(y_test),\n",
    "                'predicted_class': y_pred,\n",
    "                'actual_class': y_test,\n",
    "                \"patient_id\": info.iloc[test_idx][\"patient_id\"].to_list()\n",
    "            }\n",
    "            predictions = pd.DataFrame.from_dict(predictions_dict)\n",
    "            all_predictions = pd.concat([all_predictions, predictions])\n",
    "    \n",
    "            # Compute and save final scores for the fold\n",
    "            final_scores = calculate_metrics_statistics(test_scores)\n",
    "            df = pd.DataFrame([(key,) + values for key, values in final_scores.items()],\n",
    "                              columns=['Index', 'Mean', 'Stddev']).set_index('Index')\n",
    "            df.to_csv(\n",
    "                os.path.join(results_directory, f\"final_scores_{day_night_format}_{part}_fold\"),\n",
    "            )\n",
    "        \n",
    "            for metric, (mean, stddev) in final_scores.items():\n",
    "                print(f\"      {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "            print()\n",
    "        \n",
    "    all_predictions.to_csv(\n",
    "    os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )\n"
   ],
   "id": "56f05da8d16a7c69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: night\n",
      "544\n",
      "fold:  1\n",
      "    GBM\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[209], line 44\u001B[0m\n\u001B[0;32m     34\u001B[0m test_scores \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     36\u001B[0m model \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m     37\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mclassifiers[clf_type], \n\u001B[0;32m     38\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grids[clf_type], \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     42\u001B[0m     cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(GroupKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39msplit(X_train, y_train, info\u001B[38;5;241m.\u001B[39miloc[train_idx][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m     43\u001B[0m )\n\u001B[1;32m---> 44\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     47\u001B[0m metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(model, X_test, y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1013\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1014\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1015\u001B[0m     )\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1019\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1023\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1572\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1573\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\sklearn\\model_selection\\_search.py:965\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    957\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    958\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    959\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    960\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    961\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    962\u001B[0m         )\n\u001B[0;32m    963\u001B[0m     )\n\u001B[1;32m--> 965\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    976\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    977\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    984\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    985\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    986\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    987\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    988\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     73\u001B[0m )\n\u001B[1;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mental-disorder-ts-dt1eiCtD-py3.10\\lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "cell_type": "markdown",
   "id": "8e676172",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "id": "c879fd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T19:38:23.024388100Z",
     "start_time": "2024-12-05T17:01:58.637617Z"
    }
   },
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    predictions = pd.read_csv(os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"))\n",
    "\n",
    "    grouped = predictions.groupby(['patient_id', 'classifier'])\n",
    "\n",
    "    most_common_class = (\n",
    "        grouped['predicted_class']\n",
    "        .apply(lambda x: x.mode()[0]) \n",
    "        .reset_index(name='final_predicted_class')\n",
    "    )\n",
    "\n",
    "    final_results = pd.merge(\n",
    "        most_common_class,\n",
    "        predictions[['patient_id', 'actual_class']].drop_duplicates(),\n",
    "        on='patient_id'\n",
    "    )\n",
    "\n",
    "    final_results.to_csv(\n",
    "        os.path.join(predictions_directory, f\"final_predictions_{day_night_format}_{part}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    voting_metrics = (\n",
    "    final_results.groupby('classifier')\n",
    "    .apply(lambda group: pd.Series(\n",
    "        calculate_metrics_from_df(group['actual_class'], group['final_predicted_class'])\n",
    "    ))\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "    display(voting_metrics)\n",
    "    \n",
    "    voting_metrics.to_csv(\n",
    "    os.path.join(predictions_directory, f\"voting_scores_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: full_24h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  classifier  accuracy  balanced_accuracy        f1  precision    recall  \\\n",
       "0         LR  0.411765           0.415278  0.390244   0.432432  0.355556   \n",
       "1         RF  0.517647           0.518056  0.528736   0.547619  0.511111   \n",
       "2        SVM  0.411765           0.400000  0.519231   0.457627  0.600000   \n",
       "\n",
       "   specificity   ROC_AUC       MCC  \n",
       "0        0.475  0.415278 -0.170585  \n",
       "1        0.525  0.518056  0.036051  \n",
       "2        0.200  0.400000 -0.216647  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.415278</td>\n",
       "      <td>0.390244</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.415278</td>\n",
       "      <td>-0.170585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.518056</td>\n",
       "      <td>0.528736</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.518056</td>\n",
       "      <td>0.036051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-0.216647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: night\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  classifier  accuracy  balanced_accuracy        f1  precision    recall  \\\n",
       "0         LR  0.435294           0.433333  0.466667   0.466667  0.466667   \n",
       "1         RF  0.435294           0.434722  0.454545   0.465116  0.444444   \n",
       "2        SVM  0.470588           0.465278  0.526316   0.500000  0.555556   \n",
       "\n",
       "   specificity   ROC_AUC       MCC  \n",
       "0        0.400  0.433333 -0.133333  \n",
       "1        0.425  0.434722 -0.130339  \n",
       "2        0.375  0.465278 -0.070430  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.435294</td>\n",
       "      <td>0.434722</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.434722</td>\n",
       "      <td>-0.130339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>-0.070430</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: day\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  classifier  accuracy  balanced_accuracy        f1  precision    recall  \\\n",
       "0         LR  0.411765           0.420833  0.324324   0.413793  0.266667   \n",
       "1         RF  0.447059           0.448611  0.447059   0.475000  0.422222   \n",
       "2        SVM  0.388235           0.386111  0.422222   0.422222  0.422222   \n",
       "\n",
       "   specificity   ROC_AUC       MCC  \n",
       "0        0.575  0.420833 -0.166692  \n",
       "1        0.475  0.448611 -0.102778  \n",
       "2        0.350  0.386111 -0.227778  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>balanced_accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>ROC_AUC</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.324324</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>-0.166692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.448611</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.448611</td>\n",
       "      <td>-0.102778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.386111</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.386111</td>\n",
       "      <td>-0.227778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "42b3064f",
   "metadata": {},
   "source": [
    "## Depresjon"
   ]
  },
  {
   "cell_type": "code",
   "id": "02cd4d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:27.092728Z",
     "start_time": "2025-01-15T20:47:25.878328Z"
    }
   },
   "source": [
    "path = os.path.join(PROCESSED_DATA_DIR, DAY_WINDOWS_DIR, \"depresjon\")\n",
    "dataset = DatasetWin(dirpath=path, sep=',')\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ],
   "outputs": [],
   "execution_count": 220
  },
  {
   "cell_type": "code",
   "id": "1ed6a20b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:49.313876Z",
     "start_time": "2025-01-15T20:47:27.187723Z"
    }
   },
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "    control_df[\"class\"] = 0\n",
    "    max_patient = condition_df['patient_id'].max()\n",
    "    control_df['patient_id'] += max_patient # changing numeration of patients\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "    datasets[part] = entire_df"
   ],
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:49.471216Z",
     "start_time": "2025-01-15T20:47:49.409086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save windows features\n",
    "for part, df in datasets.items():\n",
    "    filename = f\"{DEPRESJON_PREFIX}_{day_night_format}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    df.to_csv(filepath, index=False, header=True)"
   ],
   "id": "9e4d6be9403f6df",
   "outputs": [],
   "execution_count": 222
  },
  {
   "cell_type": "markdown",
   "id": "0459fdc5",
   "metadata": {},
   "source": [
    "## Depresjon classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8ec87dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:22:22.908269Z",
     "start_time": "2025-01-15T18:22:22.900271Z"
    }
   },
   "source": [
    "dataset = DEPRESJON_PREFIX"
   ],
   "outputs": [],
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "id": "5df3de23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:22:23.205730Z",
     "start_time": "2025-01-15T18:22:23.179732Z"
    }
   },
   "source": [
    "# reading\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    filename = f\"{dataset}_{day_night_format}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).dropna()"
   ],
   "outputs": [],
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "id": "29b4f9b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-05T17:31:58.168498Z",
     "start_time": "2024-12-05T17:31:50.608969Z"
    }
   },
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "results_directory = os.path.join(\".\", MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"depresjon\")\n",
    "predictions_directory = os.path.join('.', MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"depresjon\", \"predictions\")\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "os.makedirs(predictions_directory, exist_ok=True)\n",
    "predictions = pd.DataFrame(columns=['fold', 'classifier', 'predicted_class', 'actual_class', 'patient_id'])\n",
    "\n",
    "for part in [\"night\", \"full_24h\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    X = datasets[part]\n",
    "    y = datasets[part]['class']\n",
    "    print(len(X))\n",
    "    info = X.iloc[:, -3:]\n",
    "    group_kfold = StratifiedGroupKFold(n_splits=5)\n",
    "    fold_num = 0\n",
    "    all_predictions = pd.DataFrame()\n",
    "    \n",
    "    for train_idx, test_idx in group_kfold.split(X, y, groups=X['patient_id']):\n",
    "        fold_num += 1 \n",
    "        X = X.iloc[:, :-3]\n",
    "        print(\"fold: \", fold_num)\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "        X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.005)\n",
    "        X_train, X_test = standardize(X_train, X_test)\n",
    "    \n",
    "        for clf_type in [\"GBM\", \"LR\", \"SVM\", \"RF\"]: \n",
    "            if clf_type == \"GBM\":\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "            print(f\"    {clf_type}\")\n",
    "            \n",
    "            test_scores = []\n",
    "            \n",
    "            model = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=list(StratifiedGroupKFold(n_splits=5).split(X_train, y_train, info.iloc[train_idx][\"patient_id\"]))\n",
    "            )\n",
    "            print(y_train.unique())\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            metrics = calculate_metrics(model, X_test, y_test)\n",
    "            # print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "    \n",
    "            # Save individual fold metrics\n",
    "            pd.DataFrame.from_records(test_scores).to_csv(\n",
    "                os.path.join(results_directory, f\"test_scores_{day_night_format}_{part}_fold_{clf_type}\"),\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            predictions_dict = {\n",
    "                \"fold\": [fold_num] * len(y_test),\n",
    "                'classifier': [clf_type] * len(y_test),\n",
    "                'predicted_class': y_pred,\n",
    "                'actual_class': y_test,\n",
    "                \"patient_id\": info.iloc[test_idx][\"patient_id\"].to_list()\n",
    "            }\n",
    "            predictions = pd.DataFrame.from_dict(predictions_dict)\n",
    "            all_predictions = pd.concat([all_predictions, predictions])\n",
    "    \n",
    "            # Compute and save final scores for the fold\n",
    "            final_scores = calculate_metrics_statistics(test_scores)\n",
    "            df = pd.DataFrame([(key,) + values for key, values in final_scores.items()],\n",
    "                              columns=['Index', 'Mean', 'Stddev']).set_index('Index')\n",
    "            df.to_csv(\n",
    "                os.path.join(results_directory, f\"final_scores_{day_night_format}_{part}_fold\"),\n",
    "            )\n",
    "        \n",
    "            for metric, (mean, stddev) in final_scores.items():\n",
    "                print(f\"      {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "            print()\n",
    "        \n",
    "    all_predictions.to_csv(\n",
    "    os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: night\n",
      "932\n",
      "fold:  1\n",
      "    LR\n",
      "[1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 2730 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2730 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[86], line 44\u001B[0m\n\u001B[0;32m     35\u001B[0m model \u001B[38;5;241m=\u001B[39m GridSearchCV(\n\u001B[0;32m     36\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mclassifiers[clf_type], \n\u001B[0;32m     37\u001B[0m     param_grid\u001B[38;5;241m=\u001B[39mparam_grids[clf_type], \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     41\u001B[0m     cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(StratifiedGroupKFold(n_splits\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39msplit(X_train, y_train, info\u001B[38;5;241m.\u001B[39miloc[train_idx][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatient_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[0;32m     42\u001B[0m )\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(y_train\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m---> 44\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     47\u001B[0m metrics \u001B[38;5;241m=\u001B[39m calculate_metrics(model, X_test, y_test)\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1019\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m   1013\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m   1014\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m   1015\u001B[0m     )\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m-> 1019\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1021\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m   1022\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m   1023\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1573\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1571\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1572\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1573\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:996\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    989\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m!=\u001B[39m n_candidates \u001B[38;5;241m*\u001B[39m n_splits:\n\u001B[0;32m    990\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    991\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv.split and cv.get_n_splits returned \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    992\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minconsistent results. Expected \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    993\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits, got \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_splits, \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m n_candidates)\n\u001B[0;32m    994\u001B[0m     )\n\u001B[1;32m--> 996\u001B[0m \u001B[43m_warn_or_raise_about_fit_failures\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    998\u001B[0m \u001B[38;5;66;03m# For callable self.scoring, the return type is only know after\u001B[39;00m\n\u001B[0;32m    999\u001B[0m \u001B[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001B[39;00m\n\u001B[0;32m   1000\u001B[0m \u001B[38;5;66;03m# can now be inserted with the correct key. The type checking\u001B[39;00m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001B[39;00m\n\u001B[0;32m   1002\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscoring):\n",
      "File \u001B[1;32m~\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001B[0m, in \u001B[0;36m_warn_or_raise_about_fit_failures\u001B[1;34m(results, error_score)\u001B[0m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_failed_fits \u001B[38;5;241m==\u001B[39m num_fits:\n\u001B[0;32m    523\u001B[0m     all_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mAll the \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIt is very likely that your model is misconfigured.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    526\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou can try to debug the error by setting error_score=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    527\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    528\u001B[0m     )\n\u001B[1;32m--> 529\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(all_fits_failed_message)\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    532\u001B[0m     some_fits_failed_message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mnum_failed_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m fits failed out of a total of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_fits\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe score on these train-test partitions for these parameters\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    538\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBelow are more details about the failures:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mfit_errors_summary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    539\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: \nAll the 2730 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n2730 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"C:\\Users\\jolka\\OneDrive\\Pulpit\\studia\\sem_9\\Mental-Disorder-TS\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1301, in fit\n    raise ValueError(\nValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "id": "11f7ba5c",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41fd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    predictions = pd.read_csv(os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"))\n",
    "    print(predictions)\n",
    "\n",
    "    grouped = predictions.groupby(['patient_id', 'classifier'])\n",
    "\n",
    "    most_common_class = (\n",
    "        grouped['predicted_class']\n",
    "        .apply(lambda x: x.mode()[0]) \n",
    "        .reset_index(name='final_predicted_class')\n",
    "    )\n",
    "\n",
    "    final_results = pd.merge(\n",
    "        most_common_class,\n",
    "        predictions[['patient_id', 'actual_class']].drop_duplicates(),\n",
    "        on='patient_id'\n",
    "    )\n",
    "\n",
    "    print(final_results)\n",
    "\n",
    "    final_results.to_csv(\n",
    "        os.path.join(predictions_directory, f\"final_predictions_{day_night_format}_{part}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    voting_metrics = (\n",
    "    final_results.groupby('classifier')\n",
    "    .apply(lambda group: pd.Series(\n",
    "        calculate_metrics_from_df(group['actual_class'], group['final_predicted_class'])\n",
    "    ))\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "    print(voting_metrics)\n",
    "    \n",
    "    voting_metrics.to_csv(\n",
    "    os.path.join(predictions_directory, f\"voting_scores_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24832b",
   "metadata": {},
   "source": [
    "## Psykose"
   ]
  },
  {
   "cell_type": "code",
   "id": "0496473c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:47:50.914052Z",
     "start_time": "2025-01-15T20:47:49.566210Z"
    }
   },
   "source": [
    "path = os.path.join(PROCESSED_DATA_DIR, DAY_WINDOWS_DIR, \"psykose\")\n",
    "dataset = DatasetWin(dirpath=path, sep=',')\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ],
   "outputs": [],
   "execution_count": 223
  },
  {
   "cell_type": "code",
   "id": "0537c448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:48:11.966145Z",
     "start_time": "2025-01-15T20:47:51.009047Z"
    }
   },
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "    control_df[\"class\"] = 0\n",
    "    max_patient = condition_df['patient_id'].max()\n",
    "    control_df['patient_id'] += max_patient # changing numeration of patients\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "    datasets[part] = entire_df"
   ],
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T20:48:12.122027Z",
     "start_time": "2025-01-15T20:48:12.061091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# saving windows\n",
    "for part, df in datasets.items():\n",
    "    filename = f\"{PSYKOSE_PREFIX}_{day_night_format}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    df.to_csv(filepath, index=False, header=True)"
   ],
   "id": "a721ff78b79c6bf5",
   "outputs": [],
   "execution_count": 225
  },
  {
   "cell_type": "markdown",
   "id": "275cd9ef",
   "metadata": {},
   "source": [
    "## Psykose classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "0c94464a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:29:18.393649Z",
     "start_time": "2025-01-15T18:29:18.381649Z"
    }
   },
   "source": [
    "dataset = PSYKOSE_PREFIX"
   ],
   "outputs": [],
   "execution_count": 186
  },
  {
   "cell_type": "code",
   "id": "07cac92e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T18:29:18.717745Z",
     "start_time": "2025-01-15T18:29:18.652808Z"
    }
   },
   "source": [
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    filename = f\"{dataset}_{day_night_format}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, WINDOWS_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).dropna()\n"
   ],
   "outputs": [],
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0994375",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = os.path.join(\".\", MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"psykose\")\n",
    "predictions_directory = os.path.join('.', MAIN_RESULTS_DIR, DAY_WINDOWS_DIR, \"hyperactiv\", \"predictions\")\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "os.makedirs(predictions_directory, exist_ok=True)\n",
    "predictions = pd.DataFrame(columns=['fold', 'classifier', 'predicted_class', 'actual_class', 'patient_id'])\n",
    "\n",
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    X = datasets[part]\n",
    "    y = datasets[part]['class']\n",
    "    info = X.iloc[:, -3:]\n",
    "    group_kfold = GroupKFold(n_splits=3)\n",
    "    fold_num = 0\n",
    "    for train_idx, test_idx in group_kfold.split(X, y, groups=X['patient_id']):\n",
    "        fold_num += 1 \n",
    "        X = X.iloc[:, :-3]\n",
    "        print(\"fold: \", fold_num)\n",
    "        print(len(train_idx))\n",
    "        print(len(test_idx))\n",
    "        \n",
    "        np.random.shuffle(train_idx)\n",
    "        np.random.shuffle(test_idx)\n",
    "\n",
    "        print(test_idx)\n",
    "    \n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.005)\n",
    "        X_train, X_test = standardize(X_train, X_test)\n",
    "\n",
    "        test_scores = []\n",
    "        for clf_type in [\"GBM\", \"LR\", \"SVM\", \"RF\"]: \n",
    "            if clf_type == \"GBM\":\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "            print(f\"    {clf_type}\")\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "            print(y_pred)\n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "\n",
    "            # Save individual fold metrics\n",
    "            pd.DataFrame.from_records(test_scores).to_csv(\n",
    "                os.path.join(results_directory, f\"test_scores_{day_night_format}_{part}_fold_{clf_type}\"),\n",
    "                index=False\n",
    "            )\n",
    "\n",
    "            for idx, pred in enumerate(y_pred):\n",
    "                y_test_val = y_test[test_idx[idx]]\n",
    "\n",
    "                new_row = {\n",
    "                    'fold': fold_num,\n",
    "                    'classifier': clf_type,\n",
    "                    'predicted_class': pred, \n",
    "                    'actual_class': y_test_val, \n",
    "                    'patient_id': info.loc[test_idx[idx], 'patient_id'] \n",
    "                }\n",
    "\n",
    "                predictions = pd.concat([predictions, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "        # Compute and save final scores for the fold\n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        df = pd.DataFrame([(key,) + values for key, values in final_scores.items()],\n",
    "                          columns=['Index', 'Mean', 'Stddev']).set_index('Index')\n",
    "        df.to_csv(\n",
    "            os.path.join(results_directory, f\"final_scores_{day_night_format}_{part}_fold\"),\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"      {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    predictions.to_csv(\n",
    "    os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6ab9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    predictions = pd.read_csv(os.path.join(predictions_directory, f\"predictions_{day_night_format}_{part}.csv\"))\n",
    "    print(predictions)\n",
    "\n",
    "    grouped = predictions.groupby(['patient_id', 'classifier'])\n",
    "\n",
    "    most_common_class = (\n",
    "        grouped['predicted_class']\n",
    "        .apply(lambda x: x.mode()[0]) \n",
    "        .reset_index(name='final_predicted_class')\n",
    "    )\n",
    "\n",
    "    final_results = pd.merge(\n",
    "        most_common_class,\n",
    "        predictions[['patient_id', 'actual_class']].drop_duplicates(),\n",
    "        on='patient_id'\n",
    "    )\n",
    "\n",
    "    print(final_results)\n",
    "\n",
    "    final_results.to_csv(\n",
    "        os.path.join(predictions_directory, f\"final_predictions_{day_night_format}_{part}.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    voting_metrics = (\n",
    "    final_results.groupby('classifier')\n",
    "    .apply(lambda group: pd.Series(\n",
    "        calculate_metrics_from_df(group['actual_class'], group['final_predicted_class'])\n",
    "    ))\n",
    "    .reset_index()\n",
    "    )\n",
    "\n",
    "    print(voting_metrics)\n",
    "    \n",
    "    voting_metrics.to_csv(\n",
    "    os.path.join(predictions_directory, f\"voting_scores_{day_night_format}_{part}.csv\"),\n",
    "    index=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-disorder-ts-ZKmnNWL8-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
